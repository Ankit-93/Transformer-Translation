{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example Sample #5761:\n",
      "\n",
      "ENGLISH: i do not want to talk about it anymore\n",
      "\n",
      "BENGALI: আমি এই বযাপারে আর কোনো কথা বলতে চাই না।\n"
     ]
    }
   ],
   "source": [
    "from utils.utils import *\n",
    "from utils.config import cList\n",
    "\n",
    "\n",
    "filename = 'data/ben.txt'\n",
    "eng,beng = loaddata(filename)\n",
    "\n",
    "sample = random.randint(0,len(eng))\n",
    "print(\"Example Sample #\"+str(sample)+\":\\n\")\n",
    "string = \"ENGLISH:\"\n",
    "for i in range(0,len(eng[sample])):\n",
    "    string += \" \"+eng[sample][i]\n",
    "print(string)\n",
    "    \n",
    "string = \"\\nBENGALI:\"\n",
    "for i in range(0,len(beng[sample])):\n",
    "    string += \" \"+beng[sample][i]\n",
    "print(string) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_eng=[]\n",
    "vocab_eng.append('<PAD>')\n",
    "vocab_eng.append('<EOS>')\n",
    "\n",
    "vocab_beng=[]\n",
    "vocab_beng.append('<PAD>')\n",
    "vocab_beng.append('<EOS>')\n",
    "\n",
    "#The index of vocab will serve as an integer representation of the word\n",
    "\n",
    "vectorized_eng = []\n",
    "vectorized_beng = []\n",
    "\n",
    "for i in range(len(eng)):\n",
    "    vectorized_eng_line = []\n",
    "    for word in eng[i]:\n",
    "        if word not in vocab_eng:\n",
    "            vocab_eng.append(word)\n",
    "            vectorized_eng_line.append(vocab_eng.index(word))\n",
    "        else:\n",
    "            vectorized_eng_line.append(vocab_eng.index(word))\n",
    "    vectorized_eng.append(vectorized_eng_line)\n",
    "    \n",
    "    vectorized_beng_line = []\n",
    "    for word in beng[i]:\n",
    "        if word not in vocab_beng:\n",
    "            vocab_beng.append(word)\n",
    "            vectorized_beng_line.append(vocab_beng.index(word))\n",
    "        else:\n",
    "            vectorized_beng_line.append(vocab_beng.index(word))\n",
    "    vectorized_beng.append(vectorized_beng_line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_eng = []\n",
    "contexts_eng = []\n",
    "\n",
    "words_beng = []\n",
    "contexts_beng = []\n",
    "\n",
    "words_eng.append(vocab_eng.index('<PAD>'))\n",
    "contexts_eng.append([vocab_eng.index('<EOS>'),vocab_eng.index('<PAD>')])\n",
    "words_eng.append(vocab_eng.index('<PAD>'))\n",
    "contexts_eng.append([vocab_eng.index('<PAD>'),vocab_eng.index('<PAD>')])\n",
    "\n",
    "words_beng.append(vocab_beng.index('<PAD>'))\n",
    "contexts_beng.append([vocab_beng.index('<EOS>'),vocab_beng.index('<PAD>')])\n",
    "words_beng.append(vocab_beng.index('<PAD>'))\n",
    "contexts_beng.append([vocab_beng.index('<PAD>'),vocab_beng.index('<PAD>')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(vectorized_eng)):\n",
    "    for j in range(0,len(vectorized_eng[i])):\n",
    "        context1=0\n",
    "        context2=0\n",
    "        \n",
    "        if j==0:\n",
    "            context1 = vocab_eng.index('<PAD>')\n",
    "            if j!=len(vectorized_eng[i])-1:\n",
    "                context2 = vectorized_eng[i][j+1]\n",
    "        if j==len(vectorized_eng[i])-1:\n",
    "            context2=vocab_eng.index('<EOS>')\n",
    "            if j!=0:\n",
    "                context1 = vectorized_eng[i][j-1]\n",
    "        if j>0 and j<len(vectorized_eng[i])-1:\n",
    "            context1 = vectorized_eng[i][j-1]\n",
    "            context2 = vectorized_eng[i][j+1]\n",
    "        \n",
    "        words_eng.append(vectorized_eng[i][j])\n",
    "        contexts_eng.append([context1,context2])\n",
    "    \n",
    "    rand = random.randint(0,3)\n",
    "    if rand == 1: #reduce the freuency of <EOS> for training data\n",
    "        words_eng.append(vocab_eng.index('<EOS>'))\n",
    "        context1 = vectorized_eng[i][len(vectorized_eng[i])-1]\n",
    "        context2 = vocab_eng.index('<PAD>')\n",
    "        contexts_eng.append([context1,context2])\n",
    "    \n",
    "    for j in range(0,len(vectorized_beng[i])):\n",
    "        \n",
    "        context1=0\n",
    "        context2=0\n",
    "        \n",
    "        if j==0:\n",
    "            context1 = vocab_beng.index('<PAD>')\n",
    "            if j!=len(vectorized_beng[i])-1:\n",
    "                context2 = vectorized_beng[i][j+1]\n",
    "        if j==len(vectorized_beng[i])-1:\n",
    "            context2=vocab_beng.index('<EOS>')\n",
    "            if j!=0:\n",
    "                context1 = vectorized_beng[i][j-1]\n",
    "        if j>0 and j<len(vectorized_beng[i])-1:\n",
    "            context1 = vectorized_beng[i][j-1]\n",
    "            context2 = vectorized_beng[i][j+1]\n",
    "        \n",
    "        words_beng.append(vectorized_beng[i][j])\n",
    "        contexts_beng.append([context1,context2])\n",
    "    \n",
    "    rand = random.randint(0,3)\n",
    "    if rand == 1: #reduce the freuency of <EOS> for training data\n",
    "        words_beng.append(vocab_beng.index('<EOS>'))\n",
    "        context1 = vectorized_beng[i][len(vectorized_beng[i])-1]\n",
    "        context2 = vocab_beng.index('<PAD>')\n",
    "        contexts_beng.append([context1,context2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "embd_inputs_eng = []\n",
    "embd_labels_eng = []\n",
    "for i in range(len(contexts_eng)):\n",
    "    for context in contexts_eng[i]:\n",
    "        embd_inputs_eng.append(words_eng[i])\n",
    "        embd_labels_eng.append(context)\n",
    "embd_inputs_eng = np.asarray(embd_inputs_eng,np.int32)\n",
    "embd_labels_eng = np.asarray(embd_labels_eng,np.int32)\n",
    "\n",
    "embd_inputs_beng = []\n",
    "embd_labels_beng = []\n",
    "for i in range(len(contexts_beng)):\n",
    "    for context in contexts_beng[i]:\n",
    "        embd_inputs_beng.append(words_beng[i])\n",
    "        embd_labels_beng.append(context)\n",
    "embd_inputs_beng = np.asarray(embd_inputs_beng,np.int32)\n",
    "embd_labels_beng = np.asarray(embd_labels_beng,np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "\n",
    "def generate_batch(inputs,labels,batch_size):\n",
    "    rand = random.choice((np.arange(len(inputs))),batch_size)\n",
    "    batch_inputs=[]\n",
    "    batch_labels=[]\n",
    "    for i in range(batch_size):\n",
    "        batch_inputs.append(inputs[int(rand[i])])\n",
    "        batch_labels.append(labels[int(rand[i])])\n",
    "    batch_inputs = np.asarray(batch_inputs,np.int32)\n",
    "    batch_labels = np.asarray(batch_labels,np.int32)\n",
    "    return batch_inputs,batch_labels\n",
    "\n",
    "def generate_batch(inputs, labels, batch_size):\n",
    "    rand_indices = np.random.choice(np.arange(len(inputs)), batch_size, replace=False)\n",
    "    batch_inputs = [inputs[i] for i in rand_indices]\n",
    "    batch_labels = [labels[i] for i in rand_indices]\n",
    "    return np.array(batch_inputs), np.array(batch_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\ANKITD~1\\AppData\\Local\\Temp/ipykernel_4984/668582608.py:4: The name tf.disable_v2_behavior is deprecated. Please use tf.compat.v1.disable_v2_behavior instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\compat\\v2_compat.py:108: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import math\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()\n",
    "\n",
    "#https://www.tensorflow.org/tutorials/word2vec\n",
    "embedding_size = 256\n",
    "vocabulary_size_eng = len(vocab_eng)\n",
    "vocabulary_size_beng = len(vocab_beng)\n",
    "\n",
    "# Placeholders for inputs\n",
    "train_inputs = tf.placeholder(tf.int32, shape=[batch_size])\n",
    "train_labels = tf.placeholder(tf.int32, shape=[batch_size,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\ANKITD~1\\AppData\\Local\\Temp/ipykernel_4984/2154042079.py:23: The name tf.train.GradientDescentOptimizer is deprecated. Please use tf.compat.v1.train.GradientDescentOptimizer instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "embeddings_eng = tf.Variable(\n",
    "    tf.random_uniform([vocabulary_size_eng, embedding_size], -1.0, 1.0))\n",
    "\n",
    "nce_weights_eng = tf.Variable(\n",
    "  tf.truncated_normal([vocabulary_size_eng, embedding_size],\n",
    "                      stddev=1.0 / math.sqrt(embedding_size)))\n",
    "nce_biases_eng = tf.Variable(tf.zeros([vocabulary_size_eng]))\n",
    "\n",
    "# Initializing the variables\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "embed_eng = tf.nn.embedding_lookup(embeddings_eng, train_inputs)\n",
    "# Compute the NCE loss, using a sample of the negative labels each time.\n",
    "loss = tf.reduce_mean(\n",
    "  tf.nn.nce_loss(weights=nce_weights_eng,\n",
    "                 biases=nce_biases_eng,\n",
    "                 labels=train_labels,\n",
    "                 inputs=embed_eng,\n",
    "                 num_sampled=10, \n",
    "                 num_classes=vocabulary_size_eng)) #num_sampled = no. of negative samples\n",
    "\n",
    "# We use the SGD optimizer.\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=1.0).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 0, loss = 48.628185\n",
      "Iter 500, loss = 5.674348\n",
      "Iter 1000, loss = 10.97838\n",
      "Iter 1500, loss = 3.0610094\n",
      "Iter 2000, loss = 2.8483238\n",
      "Iter 2500, loss = 2.9596174\n",
      "Iter 3000, loss = 4.0193863\n",
      "Iter 3500, loss = 2.483408\n",
      "Iter 4000, loss = 1.4370475\n",
      "Iter 4500, loss = 4.37745\n",
      "Iter 5000, loss = 1.9631996\n",
      "Iter 5500, loss = 1.5541096\n",
      "Iter 6000, loss = 1.371262\n",
      "Iter 6500, loss = 2.8038158\n",
      "Iter 7000, loss = 1.6841803\n",
      "Iter 7500, loss = 1.6769731\n",
      "Iter 8000, loss = 1.4558773\n",
      "Iter 8500, loss = 1.9024758\n",
      "Iter 9000, loss = 1.4053977\n",
      "Iter 9500, loss = 1.9241064\n",
      "Iter 10000, loss = 2.1942794\n",
      "Iter 10500, loss = 1.4123878\n",
      "Iter 11000, loss = 1.6471683\n",
      "Iter 11500, loss = 1.1698276\n",
      "Iter 12000, loss = 1.3635943\n",
      "Iter 12500, loss = 2.0478024\n",
      "Iter 13000, loss = 0.8369172\n",
      "Iter 13500, loss = 1.6161467\n",
      "Iter 14000, loss = 1.0986751\n",
      "Iter 14500, loss = 1.5317755\n",
      "Iter 15000, loss = 1.1854439\n",
      "Iter 15500, loss = 1.3736944\n",
      "Iter 16000, loss = 2.4033642\n",
      "Iter 16500, loss = 1.8743916\n",
      "Iter 17000, loss = 1.7101421\n",
      "Iter 17500, loss = 1.3648399\n",
      "Iter 18000, loss = 1.9677272\n",
      "Iter 18500, loss = 1.0690386\n",
      "Iter 19000, loss = 1.3591579\n",
      "Iter 19500, loss = 2.4116812\n",
      "Iter 20000, loss = 1.2724198\n",
      "Iter 20500, loss = 2.0901484\n",
      "Iter 21000, loss = 1.2183731\n",
      "Iter 21500, loss = 2.128039\n",
      "Iter 22000, loss = 1.2067354\n",
      "Iter 22500, loss = 1.0420787\n",
      "Iter 23000, loss = 1.6203529\n",
      "Iter 23500, loss = 1.267281\n",
      "Iter 24000, loss = 3.225856\n",
      "Iter 24500, loss = 1.3144553\n",
      "Iter 25000, loss = 1.3786118\n",
      "Iter 25500, loss = 1.6223369\n",
      "Iter 26000, loss = 1.5285027\n",
      "Iter 26500, loss = 1.5001916\n",
      "Iter 27000, loss = 1.3281806\n",
      "Iter 27500, loss = 1.1673144\n",
      "Iter 28000, loss = 1.7094325\n",
      "Iter 28500, loss = 1.6451359\n",
      "Iter 29000, loss = 1.4522183\n",
      "Iter 29500, loss = 1.6021118\n",
      "Iter 30000, loss = 1.0253409\n",
      "Iter 30500, loss = 1.8066242\n",
      "Iter 31000, loss = 1.0646824\n",
      "Iter 31500, loss = 1.2372612\n",
      "Iter 32000, loss = 0.9735552\n",
      "Iter 32500, loss = 1.0470989\n",
      "Iter 33000, loss = 1.2280768\n",
      "Iter 33500, loss = 0.8390006\n",
      "Iter 34000, loss = 1.3279591\n",
      "Iter 34500, loss = 1.4547654\n",
      "Iter 35000, loss = 1.1746018\n",
      "Iter 35500, loss = 1.2300825\n",
      "Iter 36000, loss = 1.9470009\n",
      "Iter 36500, loss = 1.7368121\n",
      "Iter 37000, loss = 1.3066112\n",
      "Iter 37500, loss = 1.2977141\n",
      "Iter 38000, loss = 1.0740895\n",
      "Iter 38500, loss = 1.5026522\n",
      "Iter 39000, loss = 1.2802603\n",
      "Iter 39500, loss = 1.3222547\n",
      "Iter 40000, loss = 1.6408682\n",
      "Iter 40500, loss = 0.90995026\n",
      "Iter 41000, loss = 1.294188\n",
      "Iter 41500, loss = 1.6591105\n",
      "Iter 42000, loss = 1.0054169\n",
      "Iter 42500, loss = 0.8998697\n",
      "Iter 43000, loss = 1.5336161\n",
      "Iter 43500, loss = 1.4452752\n",
      "Iter 44000, loss = 2.893582\n",
      "Iter 44500, loss = 1.1631992\n",
      "Iter 45000, loss = 1.4931927\n",
      "Iter 45500, loss = 1.3151181\n",
      "Iter 46000, loss = 1.2800226\n",
      "Iter 46500, loss = 1.9764838\n",
      "Iter 47000, loss = 0.989892\n",
      "Iter 47500, loss = 1.0581448\n",
      "Iter 48000, loss = 1.1541109\n",
      "Iter 48500, loss = 0.94783807\n",
      "Iter 49000, loss = 1.0278597\n",
      "Iter 49500, loss = 1.2276804\n",
      "Iter 50000, loss = 3.0401971\n",
      "Iter 50500, loss = 1.51688\n",
      "Iter 51000, loss = 1.4715624\n",
      "Iter 51500, loss = 1.179585\n",
      "Iter 52000, loss = 2.1740642\n",
      "Iter 52500, loss = 1.3912492\n",
      "Iter 53000, loss = 1.0557361\n",
      "Iter 53500, loss = 1.7137003\n",
      "Iter 54000, loss = 1.0895187\n",
      "Iter 54500, loss = 0.9781287\n",
      "Iter 55000, loss = 1.095\n",
      "Iter 55500, loss = 1.4573791\n",
      "Iter 56000, loss = 0.91040766\n",
      "Iter 56500, loss = 0.9768693\n",
      "Iter 57000, loss = 2.722705\n",
      "Iter 57500, loss = 1.2861242\n",
      "Iter 58000, loss = 1.111007\n",
      "Iter 58500, loss = 2.3633888\n",
      "Iter 59000, loss = 1.1123638\n",
      "Iter 59500, loss = 0.97908497\n",
      "Iter 60000, loss = 1.0269853\n",
      "Iter 60500, loss = 1.5335777\n",
      "Iter 61000, loss = 1.8888932\n",
      "Iter 61500, loss = 1.0805063\n",
      "Iter 62000, loss = 0.89167666\n",
      "Iter 62500, loss = 1.0178547\n",
      "Iter 63000, loss = 1.3257194\n",
      "Iter 63500, loss = 2.207356\n",
      "Iter 64000, loss = 1.2356217\n",
      "Iter 64500, loss = 1.1290808\n",
      "Iter 65000, loss = 1.240327\n",
      "Iter 65500, loss = 1.0863934\n",
      "Iter 66000, loss = 0.97661376\n",
      "Iter 66500, loss = 1.641471\n",
      "Iter 67000, loss = 1.1088246\n",
      "Iter 67500, loss = 1.6435317\n",
      "Iter 68000, loss = 1.7586067\n",
      "Iter 68500, loss = 1.0664024\n",
      "Iter 69000, loss = 1.9758701\n",
      "Iter 69500, loss = 2.584084\n",
      "Iter 70000, loss = 2.3839843\n",
      "Iter 70500, loss = 1.3945369\n",
      "Iter 71000, loss = 1.0159796\n",
      "Iter 71500, loss = 1.4474369\n",
      "Iter 72000, loss = 1.6521386\n",
      "Iter 72500, loss = 1.3799888\n",
      "Iter 73000, loss = 0.88383436\n",
      "Iter 73500, loss = 1.0959677\n",
      "Iter 74000, loss = 0.944117\n",
      "Iter 74500, loss = 1.2270468\n",
      "Iter 75000, loss = 1.217233\n",
      "Iter 75500, loss = 0.9744507\n",
      "Iter 76000, loss = 1.1067235\n",
      "Iter 76500, loss = 1.1488676\n",
      "Iter 77000, loss = 1.4487703\n",
      "Iter 77500, loss = 1.0953505\n",
      "Iter 78000, loss = 1.0097439\n",
      "Iter 78500, loss = 1.3788385\n",
      "Iter 79000, loss = 2.2305312\n",
      "Iter 79500, loss = 1.0400227\n",
      "Iter 80000, loss = 1.287322\n",
      "Iter 80500, loss = 1.0835596\n",
      "Iter 81000, loss = 0.98470205\n",
      "Iter 81500, loss = 1.0178199\n",
      "Iter 82000, loss = 0.9768209\n",
      "Iter 82500, loss = 1.3342314\n",
      "Iter 83000, loss = 1.3814578\n",
      "Iter 83500, loss = 1.1619165\n",
      "Iter 84000, loss = 0.9687524\n",
      "Iter 84500, loss = 1.2479695\n",
      "Iter 85000, loss = 1.2879364\n",
      "Iter 85500, loss = 1.1127365\n",
      "Iter 86000, loss = 1.414119\n",
      "Iter 86500, loss = 1.832866\n",
      "Iter 87000, loss = 1.1340315\n",
      "Iter 87500, loss = 1.3953271\n",
      "Iter 88000, loss = 1.115788\n",
      "Iter 88500, loss = 1.4090898\n",
      "Iter 89000, loss = 1.0507462\n",
      "Iter 89500, loss = 1.1375275\n",
      "Iter 90000, loss = 1.5261152\n",
      "Iter 90500, loss = 1.2660002\n",
      "Iter 91000, loss = 1.5709347\n",
      "Iter 91500, loss = 1.1745162\n",
      "Iter 92000, loss = 1.1172826\n",
      "Iter 92500, loss = 2.106895\n",
      "Iter 93000, loss = 1.2914746\n",
      "Iter 93500, loss = 1.2565305\n",
      "Iter 94000, loss = 1.0993918\n",
      "Iter 94500, loss = 1.1961358\n",
      "Iter 95000, loss = 1.6355128\n",
      "Iter 95500, loss = 1.1412866\n",
      "Iter 96000, loss = 1.3245585\n",
      "Iter 96500, loss = 1.0023156\n",
      "Iter 97000, loss = 1.253348\n",
      "Iter 97500, loss = 1.5058168\n",
      "Iter 98000, loss = 0.8229536\n",
      "Iter 98500, loss = 1.8505611\n",
      "Iter 99000, loss = 0.9832105\n",
      "Iter 99500, loss = 1.5130785\n",
      "Iter 100000, loss = 2.2174568\n",
      "Iter 100500, loss = 0.8291372\n",
      "Iter 101000, loss = 1.352191\n",
      "Iter 101500, loss = 1.1613328\n",
      "Iter 102000, loss = 1.067582\n",
      "Iter 102500, loss = 1.9647093\n",
      "Iter 103000, loss = 1.6044126\n",
      "Iter 103500, loss = 1.1720481\n",
      "Iter 104000, loss = 1.1883479\n",
      "Iter 104500, loss = 1.118598\n",
      "Iter 105000, loss = 0.84929925\n",
      "Iter 105500, loss = 0.8502058\n",
      "Iter 106000, loss = 1.1707113\n",
      "Iter 106500, loss = 1.5045874\n",
      "Iter 107000, loss = 0.80874705\n",
      "Iter 107500, loss = 1.3916779\n",
      "Iter 108000, loss = 1.5140996\n",
      "Iter 108500, loss = 1.1422648\n",
      "Iter 109000, loss = 1.2153437\n",
      "Iter 109500, loss = 1.2008778\n",
      "Iter 110000, loss = 1.8280518\n",
      "Iter 110500, loss = 0.99639446\n",
      "Iter 111000, loss = 0.82373774\n",
      "Iter 111500, loss = 1.3143923\n",
      "Iter 112000, loss = 1.2536064\n",
      "Iter 112500, loss = 0.83956695\n",
      "Iter 113000, loss = 1.451846\n",
      "Iter 113500, loss = 1.2282596\n",
      "Iter 114000, loss = 1.5110703\n",
      "Iter 114500, loss = 1.3510466\n",
      "Iter 115000, loss = 1.3262643\n",
      "Iter 115500, loss = 1.1957686\n",
      "Iter 116000, loss = 1.4282253\n",
      "Iter 116500, loss = 1.1138291\n",
      "Iter 117000, loss = 1.3780105\n",
      "Iter 117500, loss = 1.1455094\n",
      "Iter 118000, loss = 1.3637301\n",
      "Iter 118500, loss = 1.4185572\n",
      "Iter 119000, loss = 0.99067295\n",
      "Iter 119500, loss = 1.0688468\n",
      "Iter 120000, loss = 0.98438025\n",
      "Iter 120500, loss = 0.9138641\n",
      "Iter 121000, loss = 1.1251037\n",
      "Iter 121500, loss = 1.1853737\n",
      "Iter 122000, loss = 0.96678936\n",
      "Iter 122500, loss = 1.4634544\n",
      "Iter 123000, loss = 1.0067258\n",
      "Iter 123500, loss = 1.6119106\n",
      "Iter 124000, loss = 1.2373996\n",
      "Iter 124500, loss = 1.0620289\n",
      "Iter 125000, loss = 0.9273396\n",
      "Iter 125500, loss = 1.3272809\n",
      "Iter 126000, loss = 0.9472201\n",
      "Iter 126500, loss = 1.7684911\n",
      "Iter 127000, loss = 0.94874954\n",
      "Iter 127500, loss = 0.9712754\n",
      "Iter 128000, loss = 1.6398538\n",
      "Iter 128500, loss = 1.3932781\n",
      "Iter 129000, loss = 1.0657059\n",
      "Iter 129500, loss = 1.5867602\n",
      "Iter 130000, loss = 1.1319888\n",
      "Iter 130500, loss = 3.2912977\n",
      "Iter 131000, loss = 0.94067097\n",
      "Iter 131500, loss = 1.2197974\n",
      "Iter 132000, loss = 1.1024532\n",
      "Iter 132500, loss = 2.8369799\n",
      "Iter 133000, loss = 0.9571148\n",
      "Iter 133500, loss = 1.8376533\n",
      "Iter 134000, loss = 1.6290168\n",
      "Iter 134500, loss = 0.7419786\n",
      "Iter 135000, loss = 1.1371691\n",
      "Iter 135500, loss = 1.1313177\n",
      "Iter 136000, loss = 1.5312397\n",
      "Iter 136500, loss = 0.94861037\n",
      "Iter 137000, loss = 1.0214844\n",
      "Iter 137500, loss = 0.86110175\n",
      "Iter 138000, loss = 0.9647157\n",
      "Iter 138500, loss = 1.0490156\n",
      "Iter 139000, loss = 1.7114439\n",
      "Iter 139500, loss = 1.123671\n",
      "Iter 140000, loss = 1.0307819\n",
      "Iter 140500, loss = 1.385366\n",
      "Iter 141000, loss = 1.1817157\n",
      "Iter 141500, loss = 1.2006259\n",
      "Iter 142000, loss = 0.9622961\n",
      "Iter 142500, loss = 1.2332983\n",
      "Iter 143000, loss = 1.4324884\n",
      "Iter 143500, loss = 1.1592733\n",
      "Iter 144000, loss = 1.1374881\n",
      "Iter 144500, loss = 1.9686837\n",
      "Iter 145000, loss = 1.931147\n",
      "Iter 145500, loss = 0.9106325\n",
      "Iter 146000, loss = 1.0216532\n",
      "Iter 146500, loss = 1.0018531\n",
      "Iter 147000, loss = 1.3372079\n",
      "Iter 147500, loss = 1.0145147\n",
      "Iter 148000, loss = 1.2044008\n",
      "Iter 148500, loss = 1.6349865\n",
      "Iter 149000, loss = 1.1949799\n",
      "Iter 149500, loss = 1.1802869\n",
      "Iter 150000, loss = 0.99613625\n",
      "Iter 150500, loss = 1.0247358\n",
      "Iter 151000, loss = 1.4951565\n",
      "Iter 151500, loss = 0.9866415\n",
      "Iter 152000, loss = 1.3832297\n",
      "Iter 152500, loss = 1.0986495\n",
      "Iter 153000, loss = 1.0853416\n",
      "Iter 153500, loss = 1.4531622\n",
      "Iter 154000, loss = 1.5816224\n",
      "Iter 154500, loss = 1.0506467\n",
      "Iter 155000, loss = 1.8364267\n",
      "Iter 155500, loss = 1.3572932\n",
      "Iter 156000, loss = 1.6069095\n",
      "Iter 156500, loss = 1.3374629\n",
      "Iter 157000, loss = 1.4523201\n",
      "Iter 157500, loss = 1.0978537\n",
      "Iter 158000, loss = 1.166273\n",
      "Iter 158500, loss = 1.1263129\n",
      "Iter 159000, loss = 0.91568416\n",
      "Iter 159500, loss = 1.1017921\n",
      "Iter 160000, loss = 1.1759241\n",
      "Iter 160500, loss = 1.0290263\n",
      "Iter 161000, loss = 0.94908273\n",
      "Iter 161500, loss = 1.6188595\n",
      "Iter 162000, loss = 1.0886216\n",
      "Iter 162500, loss = 1.0777433\n",
      "Iter 163000, loss = 0.9629257\n",
      "Iter 163500, loss = 1.3485441\n",
      "Iter 164000, loss = 1.1106172\n",
      "Iter 164500, loss = 1.020472\n",
      "Iter 165000, loss = 1.3168802\n",
      "Iter 165500, loss = 1.0472817\n",
      "Iter 166000, loss = 1.4969069\n",
      "Iter 166500, loss = 1.2796264\n",
      "Iter 167000, loss = 1.5416145\n",
      "Iter 167500, loss = 1.6555315\n",
      "Iter 168000, loss = 1.8153498\n",
      "Iter 168500, loss = 1.1533461\n",
      "Iter 169000, loss = 1.0604838\n",
      "Iter 169500, loss = 1.0151274\n",
      "Iter 170000, loss = 1.3151401\n",
      "Iter 170500, loss = 1.118105\n",
      "Iter 171000, loss = 1.9342917\n",
      "Iter 171500, loss = 0.8445687\n",
      "Iter 172000, loss = 1.4760747\n",
      "Iter 172500, loss = 0.9819059\n",
      "Iter 173000, loss = 0.80974066\n",
      "Iter 173500, loss = 0.9099416\n",
      "Iter 174000, loss = 1.1701193\n",
      "Iter 174500, loss = 0.7902535\n",
      "Iter 175000, loss = 1.947779\n",
      "Iter 175500, loss = 1.1587795\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    convergence_threshold = 0.5\n",
    "    training_iters = 500*(int((len(embd_inputs_eng))/batch_size))\n",
    "    step=0\n",
    "    n=5\n",
    "    last_n_losses = np.zeros((n),np.float32)\n",
    "    while step<training_iters:\n",
    "        batch_inputs,batch_labels = generate_batch(embd_inputs_eng,embd_labels_eng,batch_size)\n",
    "        feed_dict = {train_inputs: batch_inputs, train_labels: batch_labels.reshape((-1,1))}\n",
    "        _, np_embedding_eng, cur_loss = sess.run([optimizer, embeddings_eng, loss], feed_dict=feed_dict)\n",
    "        if step %500==0:\n",
    "            print(\"Iter \"+str(step)+\", loss = \"+str(cur_loss))\n",
    "        last_n_losses[step%n]=cur_loss\n",
    "        if step>=n:\n",
    "            if np.mean(last_n_losses)<=convergence_threshold:\n",
    "                break\n",
    "        step+=1\n",
    "                \n",
    "print(\"\\nOptimization Finished\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_beng = tf.Variable(\n",
    "    tf.random_uniform([vocabulary_size_beng, embedding_size], -1.0, 1.0))\n",
    "\n",
    "nce_weights_beng = tf.Variable(\n",
    "  tf.truncated_normal([vocabulary_size_beng, embedding_size],\n",
    "                      stddev=1.0 / math.sqrt(embedding_size)))\n",
    "nce_biases_beng = tf.Variable(tf.zeros([vocabulary_size_beng]))\n",
    "\n",
    "# Initializing the variables\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "embed_beng = tf.nn.embedding_lookup(embeddings_beng, train_inputs)\n",
    "# Compute the NCE loss, using a sample of the negative labels each time.\n",
    "loss = tf.reduce_mean(\n",
    "  tf.nn.nce_loss(weights=nce_weights_beng,\n",
    "                 biases=nce_biases_beng,\n",
    "                 labels=train_labels,\n",
    "                 inputs=embed_beng,\n",
    "                 num_sampled=10, \n",
    "                 num_classes=vocabulary_size_beng)) #num_sampled = no. of negative samples\n",
    "\n",
    "# We use the SGD optimizer.\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=1.0).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    convergence_threshold = 0.5\n",
    "    training_iters = 500*(int((len(embd_inputs_beng))/batch_size))\n",
    "    step=0\n",
    "    n=5\n",
    "    last_n_losses = np.zeros((n),np.float32)\n",
    "    while step<training_iters:\n",
    "        batch_inputs,batch_labels = generate_batch(embd_inputs_beng,embd_labels_beng,batch_size)\n",
    "        feed_dict = {train_inputs: batch_inputs, train_labels: batch_labels.reshape((-1,1))}\n",
    "        _, np_embedding_beng, cur_loss = sess.run([optimizer, embeddings_beng, loss], feed_dict=feed_dict)\n",
    "        print(\"Iter \"+str(step)+\", loss = \"+str(cur_loss))\n",
    "        last_n_losses[step%n]=cur_loss\n",
    "        if step>=n:\n",
    "            if np.mean(last_n_losses)<=convergence_threshold:\n",
    "                break\n",
    "        step+=1\n",
    "                \n",
    "print(f\"\\nOptimization Finished\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shuffled_indices = np.arange(len(eng))\n",
    "np.random.shuffle(shuffled_indices)\n",
    "\n",
    "shuffled_vectorized_eng = []\n",
    "shuffled_vectorized_beng = []\n",
    "\n",
    "for i in range(len(eng)):\n",
    "    shuffled_vectorized_eng.append(vectorized_eng[shuffled_indices[i]])\n",
    "    shuffled_vectorized_beng.append(vectorized_beng[shuffled_indices[i]])\n",
    "\n",
    "train_len = int(.75*len(eng))\n",
    "val_len = int(.15*len(eng))\n",
    "\n",
    "train_eng = shuffled_vectorized_eng[0:train_len]\n",
    "train_beng = shuffled_vectorized_beng[0:train_len]\n",
    "val_eng = shuffled_vectorized_eng[train_len:val_len]\n",
    "val_beng = shuffled_vectorized_beng[train_len:val_len]\n",
    "test_eng = shuffled_vectorized_eng[train_len+val_len:]\n",
    "test_beng = shuffled_vectorized_beng[train_len+val_len:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bucket_and_batch(x,y,batch_size):\n",
    "    len_x= np.zeros((len(x)),np.int32)\n",
    "    for i in range(len(x)):\n",
    "        len_x[i] = len(x[i])\n",
    "    sorted_by_len_indices = np.flip(np.argsort(len_x),0)\n",
    "    sorted_x = []\n",
    "    sorted_y = []\n",
    "    for i in range(len(x)):\n",
    "        sorted_x.append(x[sorted_by_len_indices[i]])\n",
    "        sorted_y.append(y[sorted_by_len_indices[i]])\n",
    "    i=0\n",
    "    batches_x = []\n",
    "    batches_y = []\n",
    "    while i<len(x):\n",
    "        if i+batch_size>=len(x):\n",
    "            break\n",
    "        batch_x = []\n",
    "        batch_y = []\n",
    "        max_len_x = len(sorted_x[i])\n",
    "        len_y= np.zeros((len(y)),np.int32)\n",
    "        for j in range(i,i+batch_size):\n",
    "            len_y[j] = len(sorted_y[j])\n",
    "        max_len_y = np.amax(len_y)\n",
    "        for j in range(i,i+batch_size):\n",
    "            line=[]\n",
    "            for k1 in range(max_len_x+1): #+1 to include <EOS>\n",
    "                if k1==len(sorted_x[j]):\n",
    "                    line.append(np_embedding_eng[vocab_eng.index('<EOS>')])\n",
    "                elif k1>len(sorted_x[j]):\n",
    "                    line.append(np_embedding_eng[vocab_eng.index('<PAD>')])\n",
    "                else:\n",
    "                    line.append(np_embedding_eng[sorted_x[j][k1]])\n",
    "            batch_x.append(line)\n",
    "            line=[]\n",
    "            for k2 in range(max_len_y+1): #+1 to include <EOS>\n",
    "                if k2>len(sorted_y[j]):\n",
    "                    line.append(vocab_beng.index('<PAD>'))\n",
    "                elif k2==len(sorted_y[j]):\n",
    "                    line.append(vocab_beng.index('<EOS>'))\n",
    "                else:\n",
    "                    line.append(sorted_y[j][k2])\n",
    "            batch_y.append(line)\n",
    "        batch_x = np.asarray(batch_x,np.float32)\n",
    "        batch_y = np.asarray(batch_y,np.int32)\n",
    "        batches_x.append(batch_x)\n",
    "        batches_y.append(batch_y)\n",
    "        i+=batch_size\n",
    "    return batches_x,batches_y\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "\n",
    "train_batch_eng,train_batch_beng = bucket_and_batch(train_eng,train_beng,batch_size)\n",
    "\n",
    "val_batch_eng,val_batch_beng = bucket_and_batch(val_eng,val_beng,batch_size)\n",
    "\n",
    "test_batch_eng,test_batch_beng = bucket_and_batch(test_eng,test_beng,batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "PICK = [vocab_eng,vocab_beng,np_embedding_eng,np_embedding_beng,train_batch_eng,train_batch_beng,val_batch_eng,val_batch_beng,test_batch_eng,test_batch_beng]\n",
    "\n",
    "with open('translationPICKLE', 'wb') as fp:\n",
    "    pickle.dump(PICK, fp)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ad2bdc8ecc057115af97d19610ffacc2b4e99fae6737bb82f5d7fb13d2f2c186"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
